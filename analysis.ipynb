{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a8b3f8",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "1. Generate reliability score.\n",
    "\n",
    "2. K means based on reliability score.\n",
    "\n",
    "3. Fit models to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "import pmdarima as pm\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"choice.db\")\n",
    "\n",
    "query = \"SELECT * FROM prediction\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d313d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'posting_date': 'date',\n",
    "})\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = df.groupby(['year', 'month'])['gross_weight'].sum().reset_index()\n",
    "df_month['date'] = pd.to_datetime(df_month[['year', 'month']].assign(day=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9691b0e",
   "metadata": {},
   "source": [
    "Visualize total monthly donations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76eaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Total montly donations\")\n",
    "plt.plot(df_month['date'], df_month['gross_weight'])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c2c0e",
   "metadata": {},
   "source": [
    "Significant outliers **don't** correspond clearly to any natural disaster or recession.\n",
    "\n",
    "Instead, they can be attributed to a single donor with dozens of million pound donations. This presents a problem for time series analysis since those behaviors can't be explained through time series, e.g., seasonality, trends, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54cc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contribution mean\n",
    "\n",
    "df = pd.read_csv(\"prediction.csv\")\n",
    "print(df['gross_weight'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc908b24",
   "metadata": {},
   "source": [
    "Grab outliers within each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gross_weight'].plot(kind='hist', bins=30, edgecolor='black', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf1b11",
   "metadata": {},
   "source": [
    "Generate reliability scores for donors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0688024",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_months = 262 # since 2003\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['donor_id'], dtype=int)\n",
    "df_encoded = df_encoded.drop(['date', 'gross_weight', 'branch_code', 'storage_code'], axis=1)\n",
    "df_encoded = df_encoded.groupby(['year', 'month'], as_index=False).sum()\n",
    "\n",
    "df_encoded = df_encoded.map(lambda x: 1 if x != 0 else 0)\n",
    "df_encoded.to_csv(\"encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "donors = df['donor_id'].unique()\n",
    "\n",
    "donor_dict = {}\n",
    "\n",
    "for donor in donors:\n",
    "    if str(donor) == 'nan': continue\n",
    "    donor = 'donor_id_' + str(donor)\n",
    "    donor_dict[donor] = df_encoded[donor].sum()\n",
    "\n",
    "for donor in donor_dict.keys():\n",
    "    donor_dict[donor] /= n_months\n",
    "\n",
    "print(donor_dict)\n",
    "\n",
    "df_reliability = pd.DataFrame.from_dict(donor_dict, orient='index', columns=['reliability_score'])\n",
    "df_reliability.to_csv(\"reliability.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reliability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df['reliability_score'])\n",
    "\n",
    "e_scaled = StandardScaler().fit_transform(df) # this is super important\n",
    "\n",
    "inertia = []\n",
    "k_range = range(1, 20)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(e_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb54f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reliability = df_reliability.reset_index(names='donor_id')\n",
    "\n",
    "print(df_reliability.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3)\n",
    "km.fit(e_scaled)\n",
    "\n",
    "df_reliability['group'] = km.labels_\n",
    "\n",
    "print(km.labels_)\n",
    "print(df_reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean donor_id\n",
    "\n",
    "df_reliability['donor_id'] = df_reliability['donor_id'].str.replace('donor_id_', '', regex=False)\n",
    "print(df_reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6567ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.read_csv(\"data/prediction.csv\")\n",
    "\n",
    "df_merge = pd.merge(df_prediction, df_reliability, on='donor_id')\n",
    "\n",
    "df_merge.to_csv(\"final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49140828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'DRY': ['DRY', 'FRESH'],\n",
    "    'FROZEN': ['FROZEN'],\n",
    "    'REFRIG': ['REF', 'REFRIG', 'Refrigerated']\n",
    "}\n",
    "\n",
    "rename_map = {old: new for new, olds in rename_dict.items() for old in olds}\n",
    "\n",
    "df['storage_code'] = df['storage_code'].map(lambda x: rename_map.get(x, x))\n",
    "\n",
    "df.to_csv(\"data/final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66bfca",
   "metadata": {},
   "source": [
    "Group by food type and reliability cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storage_dict = {storage: g for storage, g in df.groupby('storage_code')}\n",
    "\n",
    "for storage in df_storage_dict.keys():\n",
    "    df_storage_dict[storage] = {group: g for group, g in df_storage_dict[storage].groupby('group')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for food in df_storage_dict.keys():\n",
    "    df_group_dict = df_storage_dict[food]\n",
    "    for group in df_group_dict.keys():\n",
    "        print(food, group, df_group_dict[group]['gross_weight'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for food in df_storage_dict.keys():\n",
    "    df_group_dict = df_storage_dict[food]\n",
    "    for group in df_group_dict.keys():\n",
    "        print(food, group, df_group_dict[group].describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ff165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean outliers\n",
    "for food in df_storage_dict.keys():\n",
    "    df_group_dict = df_storage_dict[food]\n",
    "    for group in df_group_dict.keys():\n",
    "        cluster = df_group_dict[group]\n",
    "        weight = cluster['gross_weight']\n",
    "        q1 = weight.quantile(0.25)\n",
    "        q3 = weight.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lower = q1 - 0.8 * iqr\n",
    "        upper = q3 + 2 * iqr\n",
    "\n",
    "        median = cluster['gross_weight'].median()\n",
    "\n",
    "        cluster['gross_weight'] = cluster['gross_weight'].apply(lambda x: median if x < lower or x > upper else x)\n",
    "        cluster.to_csv(f\"data/clusters/final_{food}_{group}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd24146",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"coolwarm\", 5)\n",
    "colors = [palette[label] for label in range(3)]\n",
    "\n",
    "for food in df_storage_dict.keys():\n",
    "    df_group_dict = df_storage_dict[food]\n",
    "    for group in df_group_dict.keys():\n",
    "            # also f\"data/clusters/final_{food}_{group}.csv\"\n",
    "            cluster = df_group_dict[group]\n",
    "\n",
    "            plt.scatter(cluster['date'], cluster['gross_weight'], s = 1, c = colors[group])\n",
    "\n",
    "plt.title(\"Donation Clusters\")\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Donation weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75312765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"ARIMA\": defaultdict(dict),\n",
    "    \"ETS\": defaultdict(dict),\n",
    "    \"Prophet\": defaultdict(dict)\n",
    "}\n",
    "\n",
    "for food in df_storage_dict.keys():\n",
    "    print(f\"Processing food type: {food}\")\n",
    "    df_group_dict = df_storage_dict[food]\n",
    "\n",
    "    for group in df_group_dict.keys():\n",
    "        cluster = f\"{food}_{group}\"\n",
    "        original_cluster_df = df_group_dict[group]\n",
    "\n",
    "        print(f\"  Processing cluster: {cluster} with raw shape {original_cluster_df.shape}\")\n",
    "\n",
    "        cluster_df = original_cluster_df.loc[:, ['date', 'gross_weight']].copy()\n",
    "\n",
    "        cluster_df['date'] = pd.to_datetime(cluster_df['date'])\n",
    "\n",
    "        cluster_df['year'] = cluster_df['date'].dt.year\n",
    "        cluster_df['month'] = cluster_df['date'].dt.month\n",
    "\n",
    "        cluster_df_agg = cluster_df.groupby(['year', 'month'])['gross_weight'].sum().reset_index()\n",
    "\n",
    "        cluster_df_agg['date'] = pd.to_datetime(dict(\n",
    "            year=cluster_df_agg['year'],\n",
    "            month=cluster_df_agg['month'],\n",
    "            day=1\n",
    "        ))\n",
    "\n",
    "        cluster_df_agg.set_index('date', inplace=True)\n",
    "\n",
    "        assembled_series = cluster_df_agg['gross_weight']\n",
    "\n",
    "        # fill missing\n",
    "        if not assembled_series.empty:\n",
    "            full_range = pd.date_range(start=assembled_series.index.min(), end=assembled_series.index.max(), freq='MS')\n",
    "        else:\n",
    "            print(f\"    Skipping cluster {cluster}: Aggregated series is empty.\")\n",
    "            continue\n",
    "\n",
    "        y_values_reindexed = assembled_series.reindex(full_range)\n",
    "\n",
    "        y_values_filled = y_values_reindexed.interpolate(method='linear')\n",
    "\n",
    "        # split\n",
    "        try:\n",
    "            split_point = len(y_values_filled) - 1\n",
    "            train = y_values_filled[:split_point]\n",
    "            test = y_values_filled[split_point:]\n",
    "        except Exception as e:\n",
    "             print(f\"    Error during train/test split for cluster {cluster}: {e}\")\n",
    "             continue\n",
    "        \n",
    "        # check/validate\n",
    "        if not train.empty and not test.empty:\n",
    "             print(f\"    Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "        else:\n",
    "            print(f\"    Skipping cluster {cluster}: Insufficient data for training or testing after split.\")\n",
    "            continue\n",
    "\n",
    "        # drop nan\n",
    "        train_cleaned = train.dropna()\n",
    "        test_cleaned = test.dropna()\n",
    "\n",
    "        if train_cleaned.empty or test_cleaned.empty:\n",
    "             print(f\"    Skipping cluster {cluster}: Not enough data after dropping remaining NaNs.\")\n",
    "             continue\n",
    "        else:\n",
    "            print(f\"    Cleaned Train shape: {train_cleaned.shape}, Cleaned Test shape: {test_cleaned.shape}\")\n",
    "\n",
    "        # models\n",
    "        for model_name in model_dict.keys():\n",
    "            print(f\"    Processing {model_name} for {cluster}\")\n",
    "\n",
    "            model = None\n",
    "            forecast = None\n",
    "\n",
    "            # arima\n",
    "            if model_name == \"ARIMA\":\n",
    "                try:\n",
    "                    model = pm.auto_arima(\n",
    "                        train_cleaned,\n",
    "                        start_p=1, start_q=1,\n",
    "                        max_p=5, max_q=5,\n",
    "                        seasonal=True,\n",
    "                        m=12,\n",
    "                        stepwise=True,\n",
    "                        error_action='ignore',\n",
    "                        suppress_warnings=True,\n",
    "                        n_fits=50\n",
    "                    )\n",
    "                    forecast = model.predict(len(test_cleaned))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"      ARIMA fitting failed for {cluster}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            elif model_name == \"ETS\":\n",
    "                try:\n",
    "                    model = ExponentialSmoothing(\n",
    "                        train_cleaned,\n",
    "                        trend='add',\n",
    "                        seasonal='add',\n",
    "                        seasonal_periods=12\n",
    "                    ).fit()\n",
    "                    forecast = model.forecast(len(test_cleaned))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"      ETS fitting failed for {cluster}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # prohpet\n",
    "            elif model_name == \"Prophet\":\n",
    "                try:\n",
    "                    train_prophet_df = pd.DataFrame({'ds': train_cleaned.index, 'y': train_cleaned.values})\n",
    "\n",
    "                    model = Prophet(\n",
    "                        yearly_seasonality=True,\n",
    "                        weekly_seasonality=False,\n",
    "                        daily_seasonality=False\n",
    "                    )\n",
    "                    model.fit(train_prophet_df)\n",
    "\n",
    "                    future = pd.DataFrame({'ds': test_cleaned.index})\n",
    "                    forecast_df = model.predict(future)\n",
    "\n",
    "                    fig = model.plot(forecast_df)\n",
    "\n",
    "                    ax = fig.gca()\n",
    "                    ax.set_xlabel(\"Date\")\n",
    "                    ax.set_ylabel(\"Cluster weight\")\n",
    "\n",
    "                    plt.title(cluster)  # Optional title\n",
    "                    plt.show()\n",
    "\n",
    "                    forecast = forecast_df.set_index('ds').reindex(test_cleaned.index)['yhat'].values\n",
    "                except Exception as e:\n",
    "                    print(f\"      Prophet fitting failed for {cluster}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if model is not None and forecast is not None:\n",
    "                model_dict[model_name][cluster][\"model\"] = model\n",
    "                model_dict[model_name][cluster][\"forecast\"] = forecast.tolist()\n",
    "\n",
    "                model_dict[model_name][cluster][\"stats\"] = {\n",
    "                    \"MAE\": mean_absolute_error(test_cleaned, forecast) if len(test_cleaned) == len(forecast) else np.nan,\n",
    "                    \"MAPE\": mean_absolute_percentage_error(test_cleaned, forecast) if len(test_cleaned) == len(forecast) else np.nan,\n",
    "                    \"RMSE\": np.sqrt(mean_squared_error(test_cleaned, forecast)) if len(test_cleaned) == len(forecast) else np.nan\n",
    "                }\n",
    "            else:\n",
    "                print(f\"    Did not get a valid model or forecast for {model_name} in {cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bfd09",
   "metadata": {},
   "source": [
    "Print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(model_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
